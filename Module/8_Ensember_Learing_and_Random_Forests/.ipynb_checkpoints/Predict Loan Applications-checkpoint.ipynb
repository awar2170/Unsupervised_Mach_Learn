{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d974bbf",
   "metadata": {},
   "source": [
    "## 17.8.2\n",
    "### Predict Loan Applications\n",
    "Jill now asks you to run a random forest model to make classifications. As you have done before, the first step is to prepare the data for the random forest classifier model.\n",
    "\n",
    "When we imported our dependencies to create the decision tree in the previous example, we use the \"tree\" module from the sklearn library, from sklearn import tree.\n",
    "\n",
    "For the random forest model, we'll use the \"ensemble\" module from the sklearn library. All the remaining dependencies will be the same. In the dependencies, replace from sklearn import tree with from sklearn.ensemble import RandomForestClassifier so that our dependencies look like the following.\n",
    "\n",
    "    '# Initial imports.\n",
    "    import pandas as pd\n",
    "    from path import Path\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "Next, read in your loans_data_encoded.csv file from previous exercises.\n",
    "\n",
    "    '# Loading data\n",
    "    file_path = Path(\"../Resources/loans_data_encoded.csv\")\n",
    "    df_loans = pd.read_csv(file_path)\n",
    "    df_loans.head()\n",
    "\n",
    "After the data has been loaded, we're going to preprocess data just like we did for the decision tree model.\n",
    "\n",
    "Preprocess the Data\n",
    "\n",
    "Now, we're going to walk through the preprocessing steps for the loan applications' encoded data so that we can fit our training and testing sets with the random forest model.\n",
    "\n",
    "If you do not quite remember the steps for preprocessing, add the blocks of code in your Jupyter Notebook as follows.\n",
    "\n",
    "    First, we define the features set.\n",
    "\n",
    "        # Define the features set.\n",
    "        X = df_loans.copy()\n",
    "        X = X.drop(\"bad\", axis=1)\n",
    "        X.head()\n",
    "\n",
    "    Next, we define the target set. Here, we're using the ravel() method, which performs the same procedure on our target set data as the values attribute.\n",
    "\n",
    "        # Define the target set.\n",
    "        y = df_loans[\"bad\"].ravel()\n",
    "        y[:5]\n",
    "\n",
    "    Now, we split into the training and testing sets.\n",
    "\n",
    "        # Splitting into Train and Test sets.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "    Lastly, we can create the StandardScaler instance, fit the scaler with the training set, and scale the data.\n",
    "\n",
    "        # Creating a StandardScaler instance.\n",
    "        scaler = StandardScaler()\n",
    "        # Fitting the Standard Scaler with the training data.\n",
    "        X_scaler = scaler.fit(X_train)\n",
    "\n",
    "        # Scaling the data.\n",
    "        X_train_scaled = X_scaler.transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "If you were able to do these steps without having to follow along, congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d4bf5",
   "metadata": {},
   "source": [
    "## 17.8.3\n",
    "### Fit the Model, Make Predictions, and Evaluate Results\n",
    "Now that you have prepared the data, you will put the random forest classifier model to practice, then evaluate the results.\n",
    "\n",
    "Now that we have preprocessed the data into training and testing data for both features and target sets, we can fit the random forest model, make predictions, and evaluate the model.\n",
    "\n",
    "### Fit the Random Forest Model\n",
    "\n",
    "Before we fit the random forest model to our X_train_scaledand y_train training data, we'll create a random forest instance using the random forest classifier, RandomForestClassifier().\n",
    "\n",
    "    '# Create a random forest classifier.\n",
    "    rf_model = RandomForestClassifier(n_estimators=128, random_state=78) \n",
    "\n",
    "The RandomForestClassifier takes a variety of parameters, but for our purposes we only need the n_estimators and the random_state.\n",
    "\n",
    "**note**\n",
    "Consult the sklearn documentation for additional information about the RandomForestClassifier and the parameters it takes.\n",
    "\n",
    "The n_estimators will allow us to set the number of trees that will be created by the algorithm. Generally, the higher number makes the predictions stronger and more stable, but can slow down the output because of the higher training time allocated. The best practice is to use between 64 and 128 random forests, though higher numbers are quite common despite the higher training time. For our purposes, we'll create 128 random forests.\n",
    "\n",
    "After we create the random forest instance, we need to fit the model with our training sets.\n",
    "\n",
    "    '# Fitting the model\n",
    "    rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "### Make Predictions Using the Testing Data\n",
    "\n",
    "After fitting the model, we can run the following code to make predictions using the scaled testing data:\n",
    "\n",
    "    # Making predictions using the testing data.\n",
    "    predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "The output will be similar as when the predictions were determined for the decision tree.\n",
    "\n",
    "The predictions generated by the model.\n",
    "\n",
    "### Evaluate the Model\n",
    "\n",
    "After making predictions on the scaled testing data, we analyze how well our random forest model classifies loan applications by using the confusion_matrix.\n",
    "\n",
    "    '# Calculating the confusion matrix.\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    '# Create a DataFrame from the confusion matrix.\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "    cm_df\n",
    "\n",
    "The confusion matrix in the DataFrame format after running 128 random forest models.\n",
    "\n",
    "These results are relatively the same as the decision tree model. To improve our predictions, let's increase the n_estimators to 500. After running all the code again, after changing the n_estimators to 500, our confusion matrix DataFrame is about the same as before.\n",
    "\n",
    "data-17-8-3-3-Confusion-Matrix-500-Random-Forest-Model.png\n",
    "\n",
    "Using the equation (TP + TN) / Total, we can determine our accuracy (determine how often the classifier predicts correctly) by running the following code. For this model, our accuracy score is 0.520:\n",
    "\n",
    "    '# Calculating the accuracy score.\n",
    "    acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "Lastly, we can print out the above results along with the classification report for the two classes:\n",
    "\n",
    "    '# Displaying results\n",
    "    print(\"Confusion Matrix\")\n",
    "    display(cm_df)\n",
    "    print(f\"Accuracy Score : {acc_score}\")\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "The print out of the confusion matrix, the accuracy score, and the classification report.\n",
    "\n",
    "From the confusion matrix results, the precision for the bad loan applications is low, indicating a large number of false positives, which indicates an unreliable positive classification. The recall is also low for the bad loan applications, which is indicative of a large number of false negatives. The F1 score is also low (33).\n",
    "\n",
    "In summary, this random forest model is not good at classifying fraudulent loan applications because the model's accuracy, 0.520, and F1 score are low.\n",
    "\n",
    "### Rank the Importance of Features\n",
    "\n",
    "One nice byproduct of the random forest algorithm is to rank the features by their importance, which allows us to see which features have the most impact on the decision.\n",
    "\n",
    "To calculate the feature importance, we can use thefeature_importances_attribute with the following code:\n",
    "\n",
    "    '# Calculate feature importance in the Random Forest model.\n",
    "    importances = rf_model.feature_importances_\n",
    "    importances\n",
    "\n",
    "The output from this code returns an array of scores for the features in the X_test set, whose sum equals 1.0:\n",
    "\n",
    "The importance of each feature, as numerical values.\n",
    "\n",
    "To sort the features by their importance with the column in the X_test set, we can modify our code above as follows:\n",
    "\n",
    "    '# We can sort the features by their importance.\n",
    "    sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)\n",
    "\n",
    "In the code, the sorted function will sort the zipped list of features with their column name (X.columns) in reverse order—more important features first—with reverse=True.\n",
    "\n",
    "Running this code will return the following output:\n",
    "\n",
    "Each feature is associated with its importance.\n",
    "\n",
    "Now we can clearly see which features, or columns, of the loan application are more relevant. The age and month_num of the loan application are the more relevant features.\n",
    "\n",
    "To improve this model, we can drop some of the lower ranked features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439d866",
   "metadata": {},
   "source": [
    "### Actual Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c43870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports.\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Loading data\n",
    "file_path = Path(\"../17-6-1-label_encode/Resources/loans_data_encoded.csv\")\n",
    "df_loans = pd.read_csv(file_path)\n",
    "df_loans.head()\n",
    "\n",
    "# Preprocess the Data\n",
    "\n",
    "# Define the features set.\n",
    "X = df_loans.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()\n",
    "\n",
    "# Define the target set.\n",
    "y = df_loans[\"bad\"].ravel()\n",
    "y[:5]\n",
    "\n",
    "#Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a45b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0           51           33\n",
       "Actual 1           23           18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.552\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65        84\n",
      "           1       0.35      0.44      0.39        41\n",
      "\n",
      "    accuracy                           0.55       125\n",
      "   macro avg       0.52      0.52      0.52       125\n",
      "weighted avg       0.58      0.55      0.56       125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.43280447750315343, 'age'),\n",
       " (0.32973986443922343, 'month_num'),\n",
       " (0.07997292251445517, 'term'),\n",
       " (0.05454782107242418, 'amount'),\n",
       " (0.021510631303272416, 'education_college'),\n",
       " (0.021102188881175144, 'education_High School or Below'),\n",
       " (0.01985561654170213, 'gender_male'),\n",
       " (0.018878176828577283, 'gender_female'),\n",
       " (0.018871722006693077, 'education_Bachelor'),\n",
       " (0.002716578909323729, 'education_Master or Above')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78) \n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "### Evaluate the Model\n",
    "\n",
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "### Rank the Importance of Features\n",
    "\n",
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "importances\n",
    "\n",
    "# We can sort the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab34d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
