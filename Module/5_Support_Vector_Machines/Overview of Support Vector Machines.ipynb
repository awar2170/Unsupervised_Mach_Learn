{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0101415",
   "metadata": {},
   "source": [
    "## 17.5.1\n",
    "### Overview of Support Vector Machines\n",
    "**Support vector machine (SVM)**, like logistic regression, is a binary classifier: It can categorize samples into one of two categories (for example, yes or no).\n",
    "\n",
    "To understand support vector machines, let's revisit logistic regression first. A logistic regression model evaluates the probability of an occurrence. For example, the model would take features into account (for example, an applicant's income and credit score) and decide whether to approve the application.\n",
    "\n",
    "The outcome is binary because the only possible options are to approve or to deny the loan application: If the probability is higher than 0.5, the application is classified as approved, or if the probability is less than that, the application is classified as denied. There is a strict cutoff line that divides one classification from the other:\n",
    "\n",
    "In logistic regression, a probability that exceeds 50% is classified one way, and all other values are classified as the other class.\n",
    "\n",
    "SVM also categorizes the target variable into one of two classes (for example, approved or denied). However, it differs from logistic regression in several ways. As a linear classifier, the goal of SVM is to find a line that separates the data into two classes:\n",
    "\n",
    "SVM is a binary classifier.\n",
    "\n",
    "However, there may be many different ways to draw the boundary line, as shown in the diagram below. Which boundary to choose isn't always clear from visual inspection, and choosing the wrong boundary can affect the performance of the model:\n",
    "\n",
    "A casual visual inspection of the data doesn't always make it clear how to optimally divide the classes.\n",
    "\n",
    "In a two-dimensional grid, as shown below, SVM draws a line at the edge of each class, and attempts to maximize the distance between them. It does so by separating the data points with the largest possible margins:\n",
    "\n",
    "SVM seeks to maximize the margins between the two classes\n",
    "\n",
    "A hyperplane is the line exactly between the two margins (i.e., equidistant from both margins). Again, the SVM's goal is to find the hyperplane with the widest possible margins (i.e., the largest margin of separation between the two classes):\n",
    "\n",
    "SVM seeks to find the widest equidistant margins to improve classification predictions.\n",
    "\n",
    "Support vectors are defined as the data points closest to the hyperplane:\n",
    "\n",
    "Support vectors are data points that define the class boundaries. Data points closest to the hyperplane are support vectors and serve as decision boundaries for classification.\n",
    "\n",
    "Real-life data, however, can be messy and will often not yield such a clean line of separation. Imagine that a data point belonging to the blue class were found closer to the cluster of data points that belong to the red class. In this case, would the hyperplane have to be relocated? Would the support vectors have to be redefined?\n",
    "\n",
    "TextSVMs can make exceptions for outliers with soft margins.: A data point is an outlier if it is close to a cluster of data points from another class.\n",
    "\n",
    "SVMs can accommodate such outliers by using soft margins. A soft margin allows SVM to make allowances for outliers that cross the hyperplane while maintaining support vectors and hyperplane to maximize the overall separation of the two classes:\n",
    "\n",
    "SVMs can make exceptions for outliers with soft margins.\n",
    "\n",
    "Up to this point, we have visualized using SVM in datasets with two features. A dataset with three features (e.g., age, education, income) and a target with two classes (e.g., approval or denial of a loan application) would be visualized as a 3D space, with a hyperplane separating the two classes:\n",
    "\n",
    "A 3D hyperplane separating two classes. Datasets with three features are modeled in 3D, with two target classes and a hyperplane.\n",
    "\n",
    "To summarize, SVM works by separating the two classes in a dataset with the widest possible margins. The margins, however, are soft and can make exceptions for outliers. This stands in contrast to the logistic regression model. In logistic regression, any data point whose probability of belonging to one class exceeds the cutoff point belongs to that class; all other data points belong to the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
