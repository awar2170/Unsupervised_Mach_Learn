{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Sampling\n",
    "\n",
    "Implement the SMOTEENN technique with the credit card default data. Then estimate a logistic regression model and report the classification evaluation metrics.\n",
    "\n",
    "ln_balance_limit is the log of the maximum balance they can have on the card; 1 is female, 0 male for sex; the education is denoted: 1 = graduate school; 2 = university; 3 = high school; 4 = others; 1 is married and 0 single for marriage; default_next_month is whether the person defaults in the following month (1 yes, 0 no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from path import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('../Resources/cc_default.csv')\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [i for i in df.columns if i not in ('ID', 'default_next_month')]\n",
    "X = df[x_cols]\n",
    "y = df['default_next_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ln_balance_limit', 'sex', 'education', 'marriage', 'age']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 6636, 0: 23364})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Sampling with SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 10151, 1: 7674})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the SMOTEENN technique to perform combination sampling on the data\n",
    "# Count the resampled classes\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a Logistic regression model using random undersampled data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4173, 1659],\n",
       "       [ 876,  792]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5951775616543802"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Balanced Accuracy Score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.83      0.72      0.47      0.77      0.58      0.35      5832\n",
      "          1       0.32      0.47      0.72      0.38      0.58      0.33      1668\n",
      "\n",
      "avg / total       0.71      0.66      0.53      0.68      0.58      0.34      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the imbalanced classification report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.10.3\n",
    "### Combination Sampling With SMOTEENN\n",
    "\n",
    "You discuss the results of oversampling and undersampling with Jill. When you point out to her that the improvements seem to be modest, she explains that incremental improvements are usually more realistic than drastic ones. Jill also tells you that such small improvements, in tandem with other tweaks, can add up to make a significant difference. For now, however, she suggests learning about SMOTEENN, an approach to resampling that combines aspects of both oversampling and undersampling.\n",
    "\n",
    "####\n",
    "\n",
    "As previously discussed, a downside of oversampling with SMOTE is its reliance on the immediate neighbors of a data point. Because the algorithm doesn't see the overall distribution of data, the new data points it creates can be heavily influenced by outliers. This can lead to noisy data. With downsampling, the downsides are that it involves loss of data and is not an option when the dataset is small. One way to deal with these challenges is to use a sampling strategy that is a combination of oversampling and undersampling.\n",
    "\n",
    "SMOTEENN combines the SMOTE and Edited Nearest Neighbors (ENN) algorithms. SMOTEENN is a two-step process:\n",
    "\n",
    "   1. Oversample the minority class with SMOTE.\n",
    "   2. Clean the resulting data with an undersampling strategy. If the two nearest neighbors of a data point belong to two different classes, that data point is dropped.\n",
    "\n",
    "The series of images below help illustrate the SMOTEENN technique. The first image represents a synthetically generated dataset (using the make_blobs module) and shows two classes: purple as the majority class and yellow as the minority class.\n",
    "\n",
    "Visualization of a dataset with class imbalance.\n",
    "\n",
    "In the following image, the minority class is oversampled with SMOTE.\n",
    "\n",
    "data-17-10-3-2-Oversampling-Smote.png\n",
    "\n",
    "Note that the two classes significantly overlap, as the box indicates below. This overlap makes classification difficult.\n",
    "\n",
    "Oversampling with SMOTE can create overlapping classes.\n",
    "\n",
    "In the next image, SMOTEENN is applied, instead of SMOTE. As with SMOTE, the minority class is oversampled; however, an undersampling step is added, removing some of each class's outliers from the dataset. The result is that the two classes are separated more cleanly.\n",
    "\n",
    "SMOTEENN removes outliers from the dataset after oversampling.\n",
    "\n",
    "Let's apply SMOTEENN to the credit card default dataset and compare its results.\n",
    "\n",
    "Download 17-10-3-combination_sampling.zip (Links to an external site.) **THIS IS THE ABOVE CODE**\n",
    "\n",
    "The code is much the same as before. The only difference is in using the SMOTEENN module.\n",
    "\n",
    "    import pandas as pd\n",
    "    from path import Path\n",
    "    from collections import Counter\n",
    "\n",
    "    data = Path('../Resources/cc_default.csv')\n",
    "    df = pd.read_csv(data)\n",
    "    df.head()\n",
    "\n",
    "Peview of the dataset.\n",
    "\n",
    "Again, the ID and default_next_month columns are filtered to create the features dataset. The default_next_month column is defined as the target dataset.\n",
    "\n",
    "    x_cols = [i for i in df.columns if i not in ('ID', 'default_next_month')]\n",
    "    X = df[x_cols]\n",
    "    y = df['default_next_month']\n",
    "\n",
    "The dataset is split into training and testing sets.\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "Next, we import the SMOTEENN module and create an instance of SMOTEENN, which resamples the dataset.\n",
    "\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    smote_enn = SMOTEENN(random_state=0)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "Again, we use a LogisticRegression model to generate predictions.\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "\n",
    "As before, the evaluation metrics are generated. First, the confusion_matrix is generated.\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    confusion_matrix(y_test, y_pred)\n",
    "\n",
    "Next, the balanced_accuracy_score is generated.\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "Finally, we print the classification report.\n",
    "\n",
    "    from imblearn.metrics import classification_report_imbalanced\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "\n",
    "The classification report calculates the precision, recall (sensitivity), and F1 score.\n",
    "\n",
    "Resampling with SMOTEENN did not work miracles, but some of the metrics show an improvement over undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
