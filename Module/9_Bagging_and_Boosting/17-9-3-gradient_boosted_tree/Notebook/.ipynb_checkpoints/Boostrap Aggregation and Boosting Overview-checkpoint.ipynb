{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb3eeee",
   "metadata": {},
   "source": [
    "## 17.9.1\n",
    "### Bootstrap Aggregation\n",
    "Bootstrap aggregation, or \"bagging,\" is an ensemble learning technique that combines weak learners into a strong learner. In fact, you have already seen a model that uses bootstrap aggregation as part of its algorithm: the random forest model.\n",
    "\n",
    "Jill reminds you that decision trees are prone to overfitting, meaning that the algorithm's predictions are excessively tailored to the specific dataset. When there's overfitting, a model's performance will suffer when it encounters a new dataset. One way to try to overcome this problem is with bootstrap aggregation. Let's look at how it works in more detail.\n",
    "\n",
    "Bootstrap aggregation, also called bagging, is a machine learning technique used to combine weak learners into a strong learner. Bagging is composed of two parts: bootstrapping and aggregation.\n",
    "\n",
    "#### Bootstrapping\n",
    "\n",
    "Bootstrapping is a sampling technique in which samples are randomly selected, then returned to the general pool and replaced, or put back into the general pool. As a concrete example, picture your dataset as a bag containing five wooden blocks, each labeled with the letter A, B, C, D, or E.\n",
    "\n",
    "A bag containing five wooden blocks, each labeled with a letter.\n",
    "\n",
    "Imagine that you draw samples of the dataset from this bag three times. Since each sampling is the same size as the original dataset, you must draw five blocks for each sample. To do so, you grab a wooden block randomly from the bag, and after noting which block you drew, you replace it, meaning that you put it back into the bag. Because you return the block to the bag, it's possible to draw the same block again in the next draw. You repeat the process until you have a sample whose size is the same as the original dataset. The result might appear as follows:\n",
    "\n",
    "Sample 1: A, A, A, B, D\n",
    "\n",
    "Sample 2: A, B, B, C, E\n",
    "\n",
    "Sample 3: B, C, D, D, E\n",
    "\n",
    "In our example above, each sample contains multiple occurrences of the same block. Sample 1 drew the letter A three times, Sample 2 drew the letter B twice, and Sample 3 drew the letter D twice. In other words, each observation (letter) may occur repeatedly in any given sample. In real life, this means that if your dataset were a Pandas DataFrame, a given row may occur multiple times in a sample.\n",
    "\n",
    "In summary, bootstrapping is simply a sampling technique with which a number of samples are made, and in which an observation can occur multiple times.\n",
    "\n",
    "#### Aggregation\n",
    "\n",
    "In the aggregation step, different classifiers are run, using the samples drawn in the bootstrapping stage. Each classifier is run independently of the others, and all the results are aggregated via a voting process. Each classifier will vote for a label (a prediction). The final prediction is the one with the most votes.\n",
    "\n",
    "In aggregation, each model votes toward the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecd675",
   "metadata": {},
   "source": [
    "## 17.9.2\n",
    "### Boosting\n",
    "Boosting is another technique to combine weak learners into a strong learner. However, there is a major difference between bagging and boosting. In bagging, as you have seen, multiple weak learners are combined at the same time to arrive at a combined result.\n",
    "\n",
    "In boosting, however, the weak learners are not combined at the same time. Instead, they are used sequentially, as one model learns from the mistakes of the previous model.\n",
    "\n",
    "Jill assures you that learning this ensemble learning technique will be worth your time. After all, many machine learning competitions have been won with this powerful technique.\n",
    "\n",
    "#### \n",
    "\n",
    "Like bagging, boosting is also a technique to combine a set of weak learners into a strong learner. We saw in bagging that the different models work independently of one another. In contrast, boosting trains a sequence of weak models. As shown below, each model learns from the errors of the previous model, and the models form an ensemble:\n",
    "\n",
    "Boosting is an ensemble technique in which a model corrects the errors of its predecessor.\n",
    "\n",
    "#### Adaptive Boosting\n",
    "\n",
    "The idea behind Adaptive Boosting, called AdaBoost, is easy to understand. In AdaBoost, a model is trained then evaluated. After evaluating the errors of the first model, another model is trained. This time, however, the model gives extra weight to the errors from the previous model. The purpose of this weighting is to minimize similar errors in subsequent models. Then, the errors from the second model are given extra weight for the third model. This process is repeated until the error rate is minimized:\n",
    "\n",
    "In adaptive boosting, prior errors are given greater weight.\n",
    "\n",
    "The final classifier might appear like the following:\n",
    "\n",
    "In adaptive boosting, prior errors are given greater weight.\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "Gradient boosting, like AdaBoost, is an ensemble method that works sequentially. In contrast to AdaBoost, gradient boosting does not seek to minimize errors by adjusting the weight of the errors. Instead, it follows this process:\n",
    "\n",
    "   1. A small tree (called a stump) is added to the model, and the errors are evaluated.\n",
    "   2. A second stump is added to the first and attempts to minimize the errors from the first stump. These errors are called pseudo-residuals.\n",
    "   3. A third stump is added to the first two and attempts to minimize the pseudo-residuals from the previous two.\n",
    "   4. The process is repeated until the errors are minimized as much as possible, or until a specified number of repetitions has been reached:\n",
    "\n",
    "Gradient boosting is both sequential and cumulative.\n",
    "\n",
    "In gradient boosting, the learning rate refers to how aggressively pseudo-residuals are corrected during each iteration. In general, it is preferable to begin with a lower learning rate and, if necessary, adjust the rate upward.\n",
    "\n",
    "Gradient boosting is a powerful technique that is often used in machine learning competitions.\n",
    "Â© 2020 - 2022 Trilogy Education Services, a 2U, Inc. brand. All Rights Reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See 17-9-3-gradient_boosted_tree.zip for more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
